"""Deep Agent for wiki generation with page subagents.

This module provides a Deep Agent that:
1. Autonomously explores a cloned repository
2. Generates a comprehensive wiki structure (sections and pages)
3. Delegates page content generation to specialized subagents via task()
4. Collects all page content and finalizes the complete wiki

The subagent pattern keeps each page's exploration context isolated while
the parent agent orchestrates the overall wiki creation.
"""

import asyncio
import os
from typing import Any, Dict, List, Optional

import structlog
from deepagents import create_deep_agent
from deepagents.backends import FilesystemBackend
from langchain_core.tools import tool
from pydantic import BaseModel, Field

from .page_tools import create_page_finalize_tool

logger = structlog.get_logger(__name__)


# ============================================================================
# Schema Definitions
# ============================================================================


class WikiPageInput(BaseModel):
    """Schema for a wiki page in the structure."""

    title: str = Field(description="Page title")
    slug: str = Field(description="URL-friendly page identifier")
    section: str = Field(description="Section this page belongs to")
    file_paths: List[str] = Field(description="Source files relevant to this page")
    description: str = Field(description="Brief description of page content")
    content: Optional[str] = Field(
        default=None, description="Generated page content (populated by subagent)"
    )


class FinalizeWikiInput(BaseModel):
    """Schema for finalizing the complete wiki with page contents."""

    title: str = Field(description="Wiki title")
    description: str = Field(description="Wiki description")
    pages: List[Dict[str, Any]] = Field(
        description="List of wiki pages with title, slug, section, file_paths, description, and content"
    )


# ============================================================================
# Wiki Finalization Tool
# ============================================================================


def create_finalize_tool(capture_dict: Dict[str, Any]) -> Any:
    """Create the finalize_wiki tool with closure for capturing output.

    Args:
        capture_dict: Dictionary that will be mutated to store the captured structure

    Returns:
        A langchain tool that captures the wiki structure with page contents
    """

    @tool(args_schema=FinalizeWikiInput)
    def finalize_wiki(
        title: str, description: str, pages: List[Dict[str, Any]]
    ) -> str:
        """Finalize the complete wiki with all page contents.

        Call this tool when you have:
        1. Designed the wiki structure
        2. Generated content for ALL pages using the page-generator subagent
        3. Collected all page contents

        Each page should include:
        - title, slug, section, file_paths, description
        - content: The full markdown content generated by the subagent
        """
        # Validate that pages have content
        pages_with_content = [p for p in pages if p.get("content")]
        pages_without_content = [p for p in pages if not p.get("content")]

        if pages_without_content:
            missing_titles = [p.get("title", "unknown") for p in pages_without_content]
            return (
                f"ERROR: {len(pages_without_content)} pages are missing content: {missing_titles}\n\n"
                f"You must generate content for ALL pages using the page-generator subagent "
                f"before calling finalize_wiki.\n\n"
                f"Use: task(name='page-generator', task='Generate wiki page for: [title]...')"
            )

        capture_dict["title"] = title
        capture_dict["description"] = description
        capture_dict["pages"] = pages

        logger.info(
            "Wiki finalized with page contents",
            title=title,
            page_count=len(pages),
            pages_with_content=len(pages_with_content),
        )
        return f"Wiki '{title}' finalized successfully with {len(pages)} pages."

    return finalize_wiki


# ============================================================================
# System Prompts
# ============================================================================


def get_structure_prompt(
    owner: str,
    repo: str,
    file_tree: str,
    readme_content: str,
    clone_path: str,
    use_mcp_tools: bool = True,
) -> str:
    """Generate the system prompt for the wiki generation agent.

    This prompt instructs the agent to:
    1. Explore the repository
    2. Design wiki structure
    3. Delegate page generation to subagents via task()
    4. Finalize with complete content

    Args:
        owner: Repository owner/organization
        repo: Repository name
        file_tree: ASCII file tree representation
        readme_content: README file content
        clone_path: Absolute path to the cloned repository
        use_mcp_tools: Whether MCP filesystem tools are available

    Returns:
        Formatted system prompt
    """
    if use_mcp_tools:
        exploration_instructions = f"""## Exploration Strategy
The repository is located at: {clone_path}

The file tree above already shows the complete directory structure - use it to identify files to read.

### Context-Efficient Reading (IMPORTANT)
To minimize context usage and improve efficiency, follow this reading strategy:

1. **Read File Headers First (50 lines)**
   Use `read_text_file` with `head=50` to read only the first 50 lines:
   ```
   read_text_file(path="{clone_path}\\src\\main.py", head=50)
   ```
   The first 50 lines typically contain imports, docstrings, and class/function signatures -
   enough to understand the file's purpose without loading full content.

2. **Read More Only When Needed**
   If 50 lines aren't enough to understand a file:
   - Use `head=100` or `head=150` for larger files
   - Use `tail=50` to see the end of a file (exports, main block)
   - Only read full files for small config files (< 50 lines anyway)

3. **Use search_files for Discovery**
   Find related files by pattern:
   ```
   search_files(path="{clone_path}", pattern="**/*controller*.py")
   search_files(path="{clone_path}", pattern="**/test_*.py")
   ```

IMPORTANT: Always use absolute paths starting with "{clone_path}\\" when accessing files.
Do NOT read full files unless absolutely necessary - prefer head=50 for initial exploration."""
    else:
        exploration_instructions = """## Exploration Strategy
The file tree above already shows the complete directory structure - use it to identify files to read.

1. Use `read_file` to read key files and understand the codebase:
   - Config files: package.json, pyproject.toml, Cargo.toml, setup.py, etc.
   - Entry points: main.py, index.ts, App.tsx, __init__.py, etc.
   - Core modules and their purposes
2. Use `glob` to find specific file patterns if needed (e.g., `**/*.py`)
3. Use `grep` to search for patterns like class definitions, API routes, exports"""

    return f"""You are an expert technical writer creating a comprehensive wiki for {owner}/{repo}.

## Repository
- Owner: {owner}
- Name: {repo}

## Initial File Tree
{file_tree}

## README Content
{readme_content}

{exploration_instructions}

## YOUR WORKFLOW

You must complete THREE phases:

### PHASE 1: Repository Exploration
Explore this repository to understand its architecture:
- Read config files (package.json, pyproject.toml, etc.)
- Read entry points with head=50 to understand structure
- Identify key components, patterns, and architecture

### PHASE 2: Wiki Structure Design
Design a wiki with 8-12 pages covering:
- Overview and Getting Started
- Architecture and core concepts
- Key features and functionality
- API reference (if applicable)
- Development and deployment guides

For each page, note:
- title: Descriptive page title
- slug: URL-friendly identifier (lowercase, hyphens)
- section: One of "Overview", "Architecture", "Features", "API", "Deployment", "Development"
- file_paths: List of relevant source files
- description: What this page covers

### PHASE 3: Page Content Generation (CRITICAL)
For EACH page in your structure, you MUST delegate to the page-generator subagent:

```
task(
    name="page-generator",
    task="Generate wiki page content.

    Page Title: [title]
    Page Description: [description]
    Relevant Files: [file1, file2, ...]
    Repository: {owner}/{repo}
    Repository Path: {clone_path}

    Create comprehensive documentation with:
    - <details> block listing source files
    - Introduction and overview
    - Detailed sections with code examples
    - Mermaid diagrams for architecture/flow
    - Source citations for all claims
    - At least 1000 words of content"
)
```

The subagent will return the page content. Store it with the page.

**IMPORTANT:** You MUST call task() for EVERY page before finalizing.
Each task() call generates content for ONE page.

### PHASE 4: Finalize Wiki
After ALL pages have content, call `finalize_wiki` with:
- title: Wiki title for this project
- description: One-paragraph wiki description
- pages: List of all pages WITH their generated content

## CRITICAL RULES

1. **Generate ALL pages** - Do not skip any page
2. **Use subagents** - Each page content MUST come from a task() call
3. **Include content** - Every page in finalize_wiki MUST have content
4. **Sequential generation** - Generate pages one at a time to avoid context issues

## Example Workflow

1. Read key files to understand codebase
2. Design structure with 10 pages
3. For page 1: task(name="page-generator", task="Generate wiki page for: Getting Started...")
4. Store returned content
5. For page 2: task(name="page-generator", task="Generate wiki page for: Architecture...")
6. Store returned content
7. ... repeat for all pages ...
8. finalize_wiki(title="...", description="...", pages=[all pages with content])

Now begin by exploring the repository.
"""


def get_page_subagent_prompt(clone_path: str, use_mcp_tools: bool = True) -> str:
    """Get the system prompt for the page-generator subagent.

    This is a simplified prompt - the full page context is passed via the task message.

    Args:
        clone_path: Path to cloned repository
        use_mcp_tools: Whether MCP filesystem tools are available

    Returns:
        System prompt for the page subagent
    """
    if use_mcp_tools:
        tools_section = f"""## Available Tools (MCP Filesystem)
- `read_text_file(path, head=N)`: Read file contents (use head=50 for efficiency)
- `search_files(path, pattern)`: Search for files matching a glob pattern
- `list_directory(path)`: List directory contents

### Context-Efficient Reading (IMPORTANT)
To minimize context usage and improve efficiency:

1. **Read File Headers First (50 lines)**
   Use `read_text_file` with `head=50` to read only the first 50 lines:
   ```
   read_text_file(path="{clone_path}\\src\\main.py", head=50)
   ```
   The first 50 lines typically contain imports, docstrings, and class/function signatures.

2. **Read More Only When Needed**
   If 50 lines aren't enough:
   - Use `head=100` or `head=150` for larger files
   - Use `tail=50` to see the end of a file
   - Only read full files for small config files

3. **Use search_files for Discovery**
   Find related files by pattern:
   ```
   search_files(path="{clone_path}", pattern="**/*controller*.py")
   ```

All paths must be absolute, starting with: {clone_path}"""
    else:
        tools_section = """## Available Tools (FilesystemBackend)
- `read_file(path)`: Read file contents
- `glob(pattern)`: Find files matching a glob pattern
- `grep(pattern, path)`: Search for text patterns in files

All paths should be relative to the repository root."""

    return f"""You are an expert technical writer generating wiki page documentation.

You will receive a task with:
- Page title and description
- Relevant file paths as starting hints
- Repository information

Your job:
1. Explore the relevant files using the filesystem tools
2. Write comprehensive technical documentation
3. Call finalize_page with your content

{tools_section}

## Output Requirements
- Start with <details> block listing 5+ source files
- Include introduction, detailed sections, code snippets
- Use Mermaid diagrams for architecture/flows (use graph TD, not graph LR)
- Cite sources with file:line format
- At least 1000 words

## MANDATORY: Call finalize_page
You MUST call finalize_page(title=..., content=..., source_files=[...]) to submit your work.
The task is NOT complete until you call this tool.
"""


# ============================================================================
# Subagent Configuration
# ============================================================================


def _create_page_subagent_config(
    clone_path: str,
    mcp_tools: Optional[List[Any]] = None,
) -> Dict[str, Any]:
    """Create the page-generator subagent configuration.

    Args:
        clone_path: Path to cloned repository
        mcp_tools: Optional MCP filesystem tools (same tools as parent agent)

    Returns:
        Subagent configuration dictionary
    """
    # Create page finalize tool for the subagent
    # Note: Each task() invocation creates a fresh subagent, so we create a closure
    # that will capture content. The parent agent extracts content from the
    # subagent's response.
    subagent_capture: Dict[str, Any] = {}
    finalize_tool = create_page_finalize_tool(subagent_capture)

    # Determine if MCP tools are available for prompt generation
    use_mcp_tools = mcp_tools is not None and len(mcp_tools) > 0

    subagent_config = {
        "name": "page-generator",
        "description": (
            "Generate detailed wiki page content by exploring source files. "
            "Use this for each page in the wiki structure. Provide the page "
            "title, description, and relevant file paths in the task message. "
            "The subagent will explore the files and return comprehensive documentation."
        ),
        "system_prompt": get_page_subagent_prompt(clone_path, use_mcp_tools=use_mcp_tools),
        "tools": (list(mcp_tools) if mcp_tools else []) + [finalize_tool],
    }

    logger.debug(
        "Created page-generator subagent config",
        use_mcp_tools=use_mcp_tools,
        num_tools=len(subagent_config["tools"]),
    )

    return subagent_config


# ============================================================================
# Agent Creation
# ============================================================================


def create_structure_agent(
    clone_path: str,
    owner: str,
    repo: str,
    file_tree: str,
    readme_content: str,
    model: Optional[str] = None,
    mcp_tools: Optional[List[Any]] = None,
) -> Any:
    """Create a Deep Agent configured for wiki generation with page subagents.

    This agent:
    1. Explores the repository
    2. Designs wiki structure
    3. Delegates page generation to subagents via task()
    4. Collects content and finalizes

    Args:
        clone_path: Path to the cloned repository
        owner: Repository owner/organization
        repo: Repository name
        file_tree: ASCII file tree representation
        readme_content: README content
        model: Optional model override (default: uses deepagents default)
        mcp_tools: Optional MCP filesystem tools to use instead of built-in FilesystemBackend

    Returns:
        Configured Deep Agent with page-generator subagent
    """
    # This will be populated by the finalize tool
    captured_structure: Dict[str, Any] = {}

    # Create the finalize tool with capture closure
    finalize_tool = create_finalize_tool(captured_structure)

    # Generate system prompt with MCP-aware instructions
    system_prompt = get_structure_prompt(
        owner=owner,
        repo=repo,
        file_tree=file_tree,
        readme_content=readme_content,
        clone_path=clone_path,
        use_mcp_tools=mcp_tools is not None,
    )

    # Build agent kwargs
    agent_kwargs = {
        "system_prompt": system_prompt,
    }

    # Configure page-generator subagent
    page_subagent = _create_page_subagent_config(clone_path, mcp_tools)

    # Use MCP tools if provided, otherwise fall back to FilesystemBackend
    if mcp_tools:
        logger.info(
            "Creating wiki agent with MCP tools",
            num_tools=len(mcp_tools),
            repo=f"{owner}/{repo}",
        )
        agent_kwargs["tools"] = list(mcp_tools) + [finalize_tool]
    else:
        logger.warning(
            "MCP filesystem not available, using FilesystemBackend. "
            "Note: Windows absolute paths may not work correctly."
        )
        backend = FilesystemBackend(root_dir=clone_path)
        agent_kwargs["backend"] = backend
        agent_kwargs["tools"] = [finalize_tool]

    # Configure model if specified
    if model:
        from langchain.chat_models import init_chat_model

        # Add google_genai: prefix if no provider specified
        if ":" not in model:
            model_string = f"google_genai:{model}"
            provider = "google_genai"
        else:
            model_string = model
            provider = model.split(":")[0]

        model_kwargs = {"temperature": 0}

        # Pass API key from environment based on provider
        if provider == "openai" and os.environ.get("OPENAI_API_KEY"):
            model_kwargs["api_key"] = os.environ["OPENAI_API_KEY"]
            logger.debug("Using OPENAI_API_KEY from environment")
        elif provider in ("google_genai", "google") and os.environ.get("GOOGLE_API_KEY"):
            model_kwargs["api_key"] = os.environ["GOOGLE_API_KEY"]
            logger.debug("Using GOOGLE_API_KEY from environment")
        elif provider == "anthropic" and os.environ.get("ANTHROPIC_API_KEY"):
            model_kwargs["api_key"] = os.environ["ANTHROPIC_API_KEY"]
            logger.debug("Using ANTHROPIC_API_KEY from environment")

        logger.info(
            "Initializing wiki agent",
            model=model_string,
            provider=provider,
            repo=f"{owner}/{repo}",
        )
        agent_kwargs["model"] = init_chat_model(model_string, **model_kwargs)

    # Add subagent
    agent_kwargs["subagents"] = [page_subagent]

    agent = create_deep_agent(**agent_kwargs)

    # Attach the capture dict to the agent for retrieval
    agent._structure_capture = captured_structure

    return agent


# ============================================================================
# Agent Execution
# ============================================================================


async def run_structure_agent(
    clone_path: str,
    owner: str,
    repo: str,
    file_tree: str,
    readme_content: str,
    timeout: float = 600.0,  # 10 minutes for full wiki with pages
    model: Optional[str] = None,
) -> Optional[Dict[str, Any]]:
    """Run the wiki agent and return the complete wiki with page contents.

    Args:
        clone_path: Path to the cloned repository
        owner: Repository owner/organization
        repo: Repository name
        file_tree: ASCII file tree representation
        readme_content: README content
        timeout: Maximum execution time in seconds (default 10 min for full wiki)
        model: Optional model override

    Returns:
        Complete wiki dict with structure and page contents, or None if failed
    """
    from langsmith import traceable

    from src.services.mcp_filesystem_client import MCPFilesystemClient

    # Keep native path format - MCP filesystem server handles Windows paths correctly
    # Do NOT normalize to forward slashes as this can cause path corruption
    normalized_clone_path = clone_path

    # Create per-agent MCP client
    mcp_client = None
    mcp_tools = None

    try:
        mcp_client = await MCPFilesystemClient.create_for_worker(
            worker_id=f"{owner}/{repo}",
        )
        if mcp_client:
            mcp_tools = mcp_client.get_tools()
            logger.info(
                "Created MCP filesystem client for wiki agent",
                num_tools=len(mcp_tools),
                repo=f"{owner}/{repo}",
            )
    except Exception as e:
        logger.warning(f"Could not create MCP filesystem client: {e}")

    agent = create_structure_agent(
        clone_path=normalized_clone_path,
        owner=owner,
        repo=repo,
        file_tree=file_tree,
        readme_content=readme_content,
        model=model,
        mcp_tools=mcp_tools,
    )

    # Build user message
    user_message = (
        f"Create a comprehensive wiki for {owner}/{repo}. "
        f"Explore the repository, design the structure, generate content for all pages "
        f"using the page-generator subagent, and finalize with complete content."
    )

    @traceable(name=f"wiki_agent_{owner}_{repo}", run_type="chain")
    async def _invoke_agent():
        return await agent.ainvoke(
            {"messages": [{"role": "user", "content": user_message}]},
            config={"run_name": f"wiki_agent_{owner}_{repo}"},
        )

    try:
        logger.info(
            "Starting wiki agent with subagents",
            repo=f"{owner}/{repo}",
            timeout=timeout,
            model=model,
            has_mcp_tools=mcp_tools is not None,
        )

        result = await asyncio.wait_for(_invoke_agent(), timeout=timeout)

        logger.info(
            "Wiki agent completed",
            result_keys=list(result.keys()) if result else None,
        )

        # Retrieve captured structure
        wiki = getattr(agent, "_structure_capture", {})

        if wiki and wiki.get("pages"):
            pages_with_content = [p for p in wiki["pages"] if p.get("content")]
            logger.info(
                "Wiki generated successfully",
                title=wiki.get("title"),
                total_pages=len(wiki["pages"]),
                pages_with_content=len(pages_with_content),
            )
            return wiki

        logger.warning("Agent did not produce complete wiki", captured=wiki)
        return None

    except asyncio.TimeoutError:
        logger.error(f"Wiki agent timed out after {timeout}s")
        return None
    except Exception as e:
        logger.exception(f"Wiki agent failed: {e}")
        return None
    finally:
        # Clean up MCP client
        if mcp_client:
            try:
                await mcp_client.shutdown()
            except Exception as e:
                logger.warning(f"Error shutting down MCP client: {e}")
